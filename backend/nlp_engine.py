import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

# PRE-TRAINED MODEL IMPORTS
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers not installed. Install with: pip install transformers torch")


class GathaNLPEngine:
    """
    âœ… ADVANCED NLP Engine for Indian Regional Languages
    âœ… Pre-trained IndicBERT model for contextual understanding (PRIMARY)
    âœ… Multi-strategy emotion detection with context awareness (SECONDARY)
    """

    def __init__(self):
        print("="*60)
        print("ðŸš€ Initializing Advanced NLP Engine with Pre-trained Model")
        print("="*60)
        
        # Load PRE-TRAINED MODEL for Indian Languages
        self.model = None
        self.tokenizer = None
        
        if TRANSFORMERS_AVAILABLE:
            try:
                print("ðŸ“¥ Loading IndicBERT (ai4bharat/indic-bert)...")
                print("   (First time will download ~500MB model)")
                self.tokenizer = AutoTokenizer.from_pretrained("ai4bharat/indic-bert")
                self.model = AutoModel.from_pretrained("ai4bharat/indic-bert")
                self.model.eval()  # Set to evaluation mode
                print("âœ… IndicBERT loaded successfully!")
                print("   Model: ai4bharat/indic-bert (12 Indian languages)")
            except Exception as e:
                print(f"âš ï¸ Could not load IndicBERT: {e}")
                print("   Falling back to keyword-based NLP")
        else:
            print("âš ï¸ transformers library not installed")
            print("   Install with: pip install transformers torch")
        
        # Load keyword dictionaries
        print("ðŸ“š Loading emotion lexicons...")
        self.stop_words = self._load_stop_words()
        self.vectorizer = TfidfVectorizer(
            lowercase=False,
            stop_words=None,
            max_features=1000,
            ngram_range=(1, 3)
        )
        self.emotion_keywords = self._load_emotion_keywords()
        self.emotion_word_roots = self._load_emotion_word_roots()
        self.contextual_boosters = self._load_contextual_boosters()
        self.language_indicators = self._load_language_indicators()
        
        print("âœ… NLP Engine is ready!")
        if self.model is not None:
            print("ðŸŽ¯ Using MODEL-FIRST approach: IndicBERT (70%) + Keywords (30%)")
        else:
            print("ðŸŽ¯ Using keyword-based approach")
        print("="*60 + "\n")

    def _load_stop_words(self) -> set:
        """Indian language stop words"""
        return {
            # Hindi
            'à¤”à¤°', 'à¤¯à¤¾', 'à¤®à¥‡à¤‚', 'à¤•à¥€', 'à¤•à¤¾', 'à¤¸à¥‡', 'à¤¹à¥ˆ', 'à¤¹à¥ˆà¤‚', 'à¤¥à¤¾', 'à¤¥à¥‡',
            'à¤¹à¥€', 'à¤¨à¤¹à¥€à¤‚', 'à¤•à¥‡', 'à¤•à¥‹', 'à¤œà¥‹', 'à¤•à¤¿', 'à¤­à¥€', 'à¤¤à¥‹', 'à¤ªà¤°', 'à¤‡à¤¸',
            'à¤à¤•', 'à¤µà¤¹', 'à¤¯à¤¹', 'à¤‰à¤¸', 'à¤¨à¥‡', 'à¤¥à¥€', 'à¤¹à¥‹', 'à¤ªà¥‡', 'à¤¬à¤¸', 'à¤à¤¸à¥‡',
            # Marathi
            'à¤†à¤£à¤¿', 'à¤•à¤¿à¤‚à¤µà¤¾', 'à¤®à¤§à¥à¤¯à¥‡', 'à¤šà¥‡', 'à¤š', 'à¤†à¤¹à¥‡', 'à¤¹à¥‹à¤¤à¥‡', 'à¤¹à¥‹', 'à¤†à¤¹à¥‡à¤¤',
            'à¤¯à¤¾', 'à¤¨à¤¾à¤¹à¥€', 'à¤²à¤¾', 'à¤¹à¥‚à¤¨', 'à¤•à¤°à¥‚à¤¨', 'à¤¶à¥€', 'à¤¤à¤°', 'à¤¤à¥‡', 'à¤¤à¥€', 'à¤®à¥€',
            # Tamil
            'à®®à®±à¯à®±à¯à®®à¯', 'à®…à®²à¯à®²à®¤à¯', 'à®‡à®²à¯', 'à®•à¯', 'à®†à®•', 'à®‰à®³à¯à®³', 'à®Žà®©à¯', 'à®Žà®©à¯à®±à¯',
            'à®Žà®©à¯à®±', 'à®‰à®£à¯à®Ÿà¯', 'à®‡à®°à¯à®¨à¯à®¤à®¤à¯', 'à®‡à®°à¯à®•à¯à®•à®¿à®±à®¤à¯', 'à®‡à®²à¯à®²à¯ˆ', 'à®‰à®³à¯à®³à®¤à¯',
            # Kannada
            'à²®à²¤à³à²¤à³', 'à²…à²¥à²µà²¾', 'à²‡à²¦à²°', 'à²†', 'à²ˆ', 'à²†à²—à²¿à²¦à³†', 'à²†à²—à²¿à²¦à³à²¦à³', 'à²†à²—à²¿à²°à³à²¤à³à²¤à³†',
            'à²‡à²²à³à²²', 'à²®à²§à³à²¯à³†', 'à²®à³‚à²²à²•', 'à²¤à²¨à²•', 'à²µà²°à³†à²—à³†', 'à²‡à²¦à³', 'à²…à²¦à³',
            # Bengali
            'à¦à¦¬à¦‚', 'à¦¬à¦¾', 'à¦¯à§‡', 'à¦¯à¦¾', 'à¦à¦°', 'à¦à¦‡', 'à¦¸à§‡à¦‡', 'à¦¹à¦¯à¦¼', 'à¦¹à¦¯à¦¼à§‡à¦›à§‡', 'à¦›à¦¿à¦²',
            'à¦¹à¦¬à§‡', 'à¦•à¦°à§‡', 'à¦•à¦°à§‡à¦›à§‡', 'à¦¨à§‡à¦‡', 'à¦†à¦›à§‡', 'à¦¥à¦¾à¦•à§‡', 'à¦¦à§à¦¬à¦¾à¦°à¦¾', 'à¦¥à§‡à¦•à§‡'
        }

    def _load_emotion_keywords(self) -> Dict[str, List[str]]:
        """MASSIVELY EXPANDED emotion keywords"""
        return {
            'romance': [
                'à¦ªà§à¦°à§‡à¦®', 'à¤ªà¥à¤¯à¦¾à¦°', 'à¤¦à¤¿à¤²', 'à¤¹à¥ƒà¤¦à¤¯', 'à¤ªà¥à¤°à¤¿à¤¯', 'à¤®à¥‹à¤¹à¤¬à¥à¤¬à¤¤', 'à¤‡à¤¶à¥à¤•', 'à¤ªà¥à¤°à¥‡à¤®à¥€', 'à¤ªà¥à¤°à¥‡à¤®à¤¿à¤•à¤¾',
                'à¤šà¥à¤‚à¤¬à¤¨', 'à¤†à¤²à¤¿à¤‚à¤—à¤¨', 'à¤°à¥‹à¤®à¤¾à¤‚à¤¸', 'à¤ªà¥à¤°à¤£à¤¯', 'à¤¸à¥à¤¨à¥‡à¤¹', 'à¤®à¥à¤¹à¤¬à¥à¤¬à¤¤', 'à¤°à¤¤à¤¿', 'à¤•à¤¾à¤®à¤¦à¥‡à¤µ',
                'à®•à®¾à®¤à®²à¯', 'à®‡à®¤à®¯à®®à¯', 'à®…à®©à¯à®ªà¯', 'à®†à®šà¯ˆ', 'à®•à®¾à®¤à®²à®©à¯', 'à®•à®¾à®¤à®²à®¿', 'à®¨à¯‡à®šà®®à¯', 'à®®à¯à®¤à¯à®¤à®®à¯', 'à®…à®£à¯ˆà®ªà¯à®ªà¯',
                'à²ªà³à²°à³‡à²®', 'à²¹à³ƒà²¦à²¯', 'à²°à²¾à²—', 'à²ªà³à²°à²¿à²¯', 'à²ªà³à²°à³‡à²®à²¿', 'à²ªà³à²°à³€à²¤à²¿', 'à²®à²¦à³à²µà³†',
                'à¤ªà¥à¤°à¥‡à¤®', 'à¤¹à¥ƒà¤¦à¤¯', 'à¤ªà¥à¤°à¤¿à¤¯', 'à¤®à¥ˆà¤¤à¥à¤°à¥€', 'à¤ªà¥à¤°à¥‡à¤®à¤³', 'à¤ªà¥à¤°à¥‡à¤®à¤¾à¤¤',
                'à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¾', 'à¦¹à§ƒà¦¦à¦¯à¦¼', 'à¦ªà§à¦°à¦¿à¦¯à¦¼', 'à¦ªà§à¦°à§‡à¦®', 'à¦ªà§à¦°à§‡à¦®à¦¿à¦•', 'à¦­à¦¾à¦²à¦¬à¦¾à¦¸à¦¾', 'à¦šà§à¦®à§à¦¬à¦¨'
            ],
            'melancholy': [
                'à¤¦à¤°à¥à¤¦', 'à¤ªà¥€à¤¡à¤¼à¤¾', 'à¤¦à¥à¤–', 'à¤¶à¥‹à¤•', 'à¤µà¤¿à¤¯à¥‹à¤—', 'à¤–à¥‡à¤¦', 'à¤µà¥‡à¤¦à¤¨à¤¾', 'à¤‰à¤¦à¤¾à¤¸à¥€', 'à¤†à¤‚à¤¸à¥‚', 'à¤°à¥‹à¤¨à¤¾',
                'à¤—à¤®', 'à¤‰à¤¦à¤¾à¤¸', 'à¤¦à¥à¤–à¥€', 'à¤…à¤•à¥‡à¤²à¤¾', 'à¤¤à¤¨à¥à¤¹à¤¾', 'à¤•à¤°à¥à¤£', 'à¤µà¤¿à¤·à¤¾à¤¦', 'à¤—à¤°à¥€à¤¬à¥€', 'à¤¸à¤‚à¤˜à¤°à¥à¤·',
                'à¤°à¥à¤¦à¤¨', 'à¤•à¥à¤°à¤‚à¤¦à¤¨', 'à¤µà¤¿à¤²à¤¾à¤ª', 'à¤•à¤°à¥à¤£à¤¾', 'à¤…à¤¶à¥à¤°à¥', 'à¤°à¥à¤—à¥à¤£', 'à¤¨à¤¿à¤°à¤¾à¤¶',
                'à®µà®°à¯à®¤à¯à®¤à®®à¯', 'à®¤à¯à®©à¯à®ªà®®à¯', 'à®µà¯‡à®¤à®©à¯ˆ', 'à®šà¯‹à®•à®®à¯', 'à®•à®£à¯à®£à¯€à®°à¯', 'à®¤à®©à®¿à®®à¯ˆ', 'à®µà®²à®¿', 'à®¤à¯à®•à¯à®•à®®à¯', 'à®…à®´à¯à®•à¯ˆ',
                'à²µà²¿à²·à²¾à²¦', 'à²¦à³à²ƒà²–', 'à²¨à³‹à²µà³', 'à²¬à³‹à²§', 'à²•à²£à³à²£à³€à²°à³', 'à²µà³‡à²¦à²¨à³†', 'à²¦à³à²ƒà²–à²¿à²¤', 'à²…à²³à³',
                'à¤¦à¥à¤ƒà¤–', 'à¤µà¥‡à¤¦à¤¨à¤¾', 'à¤¶à¥‹à¤•', 'à¤†à¤‚à¤¸à¥‚', 'à¤¦à¥à¤–à¤£à¥‡', 'à¤¦à¥à¤ƒà¤–à¥€', 'à¤°à¤¡à¤£à¥‡',
                'à¦¦à§à¦ƒà¦–', 'à¦¬à§‡à¦¦à¦¨à¦¾', 'à¦¬à§à¦¯à¦¥à¦¾', 'à¦¶à§‹à¦•', 'à¦•à¦¾à¦¨à§à¦¨à¦¾', 'à¦à¦•à¦¾à¦•à§€à¦¤à§à¦¬', 'à¦¬à¦¿à¦·à¦¾à¦¦', 'à¦¦à¦°à¦¿à¦¦à§à¦°à¦¤à¦¾', 'à¦°à§‹à¦¨à¦¾'
            ],
            'peace': [
                'à¤¶à¤¾à¤‚à¤¤à¤¿', 'à¤¶à¤¾à¤¨à¥à¤¤à¤¿', 'à¤¸à¥à¤–', 'à¤†à¤¨à¤‚à¤¦', 'à¤¸à¤®à¤¾à¤§à¤¿', 'à¤¶à¤¾à¤‚à¤¤à¤¤à¤¾', 'à¤¨à¤¿à¤°à¥à¤µà¤¿à¤•à¤¾à¤°', 'à¤šà¥ˆà¤¨', 'à¤¸à¥à¤•à¥‚à¤¨',
                'à¤¶à¤¾à¤‚à¤¤', 'à¤®à¥Œà¤¨', 'à¤¨à¤¿à¤°à¥à¤®à¤²', 'à¤¸à¥à¤¥à¤¿à¤°', 'à¤ªà¥à¤°à¤¶à¤¾à¤‚à¤¤', 'à¤µà¤¿à¤¶à¥à¤°à¤¾à¤®', 'à¤¸à¥à¤–à¤¦',
                'à®…à®®à¯ˆà®¤à®¿', 'à®šà®®à®¾à®¤à®¿', 'à®‡à®šà¯ˆ', 'à®¨à®¿à®²à¯ˆ', 'à®…à®®à¯ˆà®¤à®¿à®¯à®¾à®©', 'à®šà®¾à®¨à¯à®¤à®®à¯', 'à®¨à®¿à®®à¯à®®à®¤à®¿', 'à®®à¯Œà®©à®®à¯',
                'à²¸à²®à²¾à²§à²¿', 'à²¶à²¾à²‚à²¤à²¿', 'à²¸à³à²–', 'à²šà²¿à²¤à³à²¤à²¸à³à²¥à²¿à²°à²¤à³†', 'à²¶à²¾à²‚à²¤', 'à²¨à²¿à²®à³à²®à²¦à²¿', 'à²®à³Œà²¨',
                'à¤¶à¤¾à¤‚à¤¤à¤¿', 'à¤¶à¤¾à¤‚à¤¤à¤¤à¤¾', 'à¤¸à¥à¤–', 'à¤¸à¤®à¤¾à¤§à¤¾à¤¨', 'à¤¶à¤¾à¤‚à¤¤', 'à¤¸à¥à¤µà¤¸à¥à¤¥',
                'à¦¶à¦¾à¦¨à§à¦¤à¦¿', 'à¦†à¦¨à¦¨à§à¦¦', 'à¦¸à¦®à¦¾à¦§à¦¿', 'à¦¨à§€à¦°à¦¬à¦¤à¦¾', 'à¦ªà§à¦°à¦¶à¦¾à¦¨à§à¦¤à¦¿', 'à¦¶à¦¾à¦¨à§à¦¤', 'à¦®à§Œà¦¨'
            ],
            'joy': [
                'à¤†à¤¨à¤‚à¤¦', 'à¤–à¥à¤¶à¥€', 'à¤¹à¤°à¥à¤·', 'à¤ªà¥à¤°à¤¸à¤¨à¥à¤¨', 'à¤‰à¤²à¥à¤²à¤¾à¤¸', 'à¤ªà¥à¤°à¤®à¥‹à¤¦', 'à¤®à¥à¤¸à¥à¤•à¤¾à¤¨', 'à¤¹à¤à¤¸à¥€', 'à¤–à¥à¤¶',
                'à¤ªà¥à¤°à¤¸à¤¨à¥à¤¨à¤¤à¤¾', 'à¤†à¤¨à¤¨à¥à¤¦', 'à¤®à¥à¤¸à¥à¤•à¥à¤°à¤¾à¤¹à¤Ÿ', 'à¤¹à¤°à¥à¤·à¤¿à¤¤', 'à¤ªà¥à¤°à¤«à¥à¤²à¥à¤²', 'à¤ªà¥à¤°à¤«à¥à¤²à¥à¤²à¤¿à¤¤', 'à¤¹à¤¾à¤¸à¥à¤¯',
                'à®šà®¨à¯à®¤à¯‹à®·à®®à¯', 'à®†à®©à®¨à¯à®¤à®®à¯', 'à®®à®•à®¿à®´à¯à®šà¯à®šà®¿', 'à®‰à®³à¯à®³à®®à¯', 'à®šà®¿à®°à®¿à®ªà¯à®ªà¯', 'à®®à®•à®¿à®´à¯à®µà¯', 'à®šà®¨à¯à®¤à¯‹à®·', 'à®¨à®•à¯ˆ',
                'à²†à²¨à²‚à²¦', 'à²¸à²‚à²¤à³‹à²·', 'à²¹à²°à³à²·', 'à²ªà³à²°à²«à³à²²à³à²²à²¤à³†', 'à²¨à²—à³', 'à²¸à²‚à²¤à²¸', 'à²–à³à²·à²¿', 'à²¨à²—à³†',
                'à¤†à¤¨à¤‚à¤¦', 'à¤–à¥à¤¶à¥€', 'à¤¹à¤°à¥à¤·', 'à¤ªà¥à¤°à¤¸à¤¨à¥à¤¨à¤¤à¤¾', 'à¤†à¤¨à¤‚à¤¦à¥€', 'à¤¹à¤¸à¤£à¥‡',
                'à¦†à¦¨à¦¨à§à¦¦', 'à¦–à§à¦¶à¦¿', 'à¦¹à§ƒà¦·à§à¦Ÿ', 'à¦¹à¦¾à¦¸à¦¿', 'à¦¸à§à¦–', 'à¦†à¦¨à¦¨à§à¦¦à¦¿à¦¤', 'à¦‰à¦²à§à¦²à¦¾à¦¸'
            ],
            'inspiration': [
                'à¤¸à¤¾à¤¹à¤¸', 'à¤¶à¤•à¥à¤¤à¤¿', 'à¤¸à¤‚à¤•à¤²à¥à¤ª', 'à¤†à¤¶à¤¾', 'à¤ªà¥à¤°à¥‡à¤°à¤£à¤¾', 'à¤‰à¤¤à¥à¤¸à¤¾à¤¹', 'à¤¦à¥ƒà¤¢à¤¼à¤¤à¤¾', 'à¤µà¥€à¤°', 'à¤¹à¤¿à¤®à¥à¤®à¤¤', 'à¤¬à¤²',
                'à¤¬à¤¹à¤¾à¤¦à¥à¤°', 'à¤µà¥€à¤°à¤¤à¤¾', 'à¤¨à¤¿à¤°à¥à¤­à¤¯', 'à¤¸à¤¾à¤¹à¤¸à¥€', 'à¤¤à¤¾à¤•à¤¤', 'à¤‰à¤®à¥à¤®à¥€à¤¦', 'à¤ªà¤°à¤¾à¤•à¥à¤°à¤®', 'à¤¤à¥‡à¤œ',
                'à®¤à¯ˆà®°à®¿à®¯à®®à¯', 'à®šà®•à¯à®¤à®¿', 'à®¨à®®à¯à®ªà®¿à®•à¯à®•à¯ˆ', 'à®šà¯à®¤à®¨à¯à®¤à®¿à®°à®®à¯', 'à®µà¯€à®°à®®à¯', 'à®‰à®¤à¯à®µà¯‡à®•à®®à¯', 'à®ªà®²à®®à¯', 'à®¤à¯ˆà®°à®¿à®¯à®®à®¾à®©',
                'à²¹à²¿à²®à³à²®à²¤à³à²¤à³', 'à²¶à²•à³à²¤à²¿', 'à²†à²¶à³†', 'à²ªà³à²°à³‡à²°à²£à³†', 'à²§à³ˆà²°à³à²¯', 'à²‰à²¤à³à²¸à²¾à²¹', 'à²¬à²²', 'à²µà³€à²°à²¤à³†',
                'à¤§à¥ˆà¤°à¥à¤¯', 'à¤¶à¤•à¥à¤¤à¥€', 'à¤ªà¥à¤°à¥‡à¤°à¤£à¤¾', 'à¤‰à¤¤à¥à¤¸à¤¾à¤¹', 'à¤¸à¤¾à¤¹à¤¸', 'à¤µà¥€à¤°',
                'à¦¸à¦¾à¦¹à¦¸', 'à¦¶à¦•à§à¦¤à¦¿', 'à¦†à¦¶à¦¾', 'à¦‰à¦¦à§à¦¦à§€à¦ªà¦¨à¦¾', 'à¦¬à§€à¦°à¦¤à§à¦¬', 'à¦ªà§à¦°à§‡à¦°à¦£à¦¾', 'à¦¸à¦¾à¦¹à¦¸à§€', 'à¦¬à¦²'
            ],
            'wisdom': [
                'à¤œà¥à¤žà¤¾à¤¨', 'à¤¸à¤¤à¥à¤¯', 'à¤¬à¥à¤¦à¥à¤§à¤¿', 'à¤µà¤¿à¤µà¥‡à¤•', 'à¤¦à¤°à¥à¤¶à¤¨', 'à¤¤à¤¤à¥à¤¤à¥à¤µ', 'à¤œà¥à¤žà¤¾à¤¨à¥€', 'à¤µà¤¿à¤¦à¥à¤¯à¤¾', 'à¤¸à¤®à¤', 'à¤ªà¥à¤°à¤œà¥à¤žà¤¾',
                'à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¾à¤¨', 'à¤µà¤¿à¤¦à¥à¤µà¤¾à¤¨', 'à¤¤à¤¤à¥à¤µ', 'à¤µà¥‡à¤¦', 'à¤¶à¤¾à¤¸à¥à¤¤à¥à¤°', 'à¤®à¤‚à¤¤à¥à¤°', 'à¤¯à¥‹à¤—', 'à¤§à¥à¤¯à¤¾à¤¨', 'à¤šà¤¿à¤‚à¤¤à¤¨',
                'à®…à®±à®¿à®µà¯', 'à®‰à®£à¯à®®à¯ˆ', 'à®¤à®¤à¯à®¤à¯à®µà®®à¯', 'à®žà®¾à®©à®®à¯', 'à®…à®±à®¿à®µà¯à®°à¯ˆ', 'à®žà®¾à®©à®¿', 'à®…à®±à®¿à®µà®¾à®³à®¿', 'à®¤à®¿à®¯à®¾à®©à®®à¯',
                'à²œà³à²žà²¾à²¨', 'à²¸à²¤à³à²¯', 'à²¤à²¤à³à²µ', 'à²¬à³à²¦à³à²§à²¿', 'à²œà³à²žà²¾à²¨à²¿', 'à²µà²¿à²¦à³à²¯à³†', 'à²¬à³à²¦à³à²§à²¿à²µà²‚à²¤', 'à²§à³à²¯à²¾à²¨',
                'à¤œà¥à¤žà¤¾à¤¨', 'à¤¸à¤¤à¥à¤¯', 'à¤¤à¤¤à¥à¤¤à¥à¤µà¤œà¥à¤žà¤¾à¤¨', 'à¤¬à¥à¤¦à¥à¤§à¥€', 'à¤œà¥à¤žà¤¾à¤¨à¥€', 'à¤µà¤¿à¤¦à¥à¤µà¤¾à¤¨',
                'à¦œà§à¦žà¦¾à¦¨', 'à¦¸à¦¤à§à¦¯', 'à¦ªà§à¦°à¦œà§à¦žà¦¾', 'à¦¬à§‹à¦§', 'à¦œà§à¦žà¦¾à¦¨à§€', 'à¦¬à¦¿à¦¦à§à¦¯à¦¾', 'à¦œà§à¦žà¦¾à¦¨à¦¬à¦¾à¦¨', 'à¦§à§à¦¯à¦¾à¦¨'
            ],
            'devotion': [
                'à¤­à¤•à¥à¤¤à¤¿', 'à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸', 'à¤ªà¥‚à¤œà¤¾', 'à¤ˆà¤¶à¥à¤µà¤°', 'à¤ªà¤µà¤¿à¤¤à¥à¤°', 'à¤§à¥à¤¯à¤¾à¤¨', 'à¤¸à¤®à¤°à¥à¤ªà¤£', 'à¤¦à¥‡à¤µà¤¤à¤¾', 'à¤ªà¥à¤°à¤¾à¤°à¥à¤¥à¤¨à¤¾', 'à¤­à¤—à¤µà¤¾à¤¨',
                'à¤†à¤°à¤¾à¤§à¤¨à¤¾', 'à¤­à¤œà¤¨', 'à¤®à¤‚à¤¦à¤¿à¤°', 'à¤§à¤°à¥à¤®', 'à¤­à¤•à¥à¤¤', 'à¤ªà¥‚à¤œà¥à¤¯', 'à¤¦à¥ˆà¤µà¥€à¤¯', 'à¤†à¤¸à¥à¤¥à¤¾', 'à¤¶à¥à¤°à¤¦à¥à¤§à¤¾',
                'à®ªà®•à¯à®¤à®¿', 'à®µà®¿à®šà¯à®µà®¾à®šà®®à¯', 'à®ªà¯‚à®œà¯ˆ', 'à®†à®©à¯à®®à¯€à®•', 'à®¤à¯†à®¯à¯à®µà®®à¯', 'à®¤à¯†à®¯à¯à®µà¯€à®•', 'à®µà®´à®¿à®ªà®¾à®Ÿà¯', 'à®•à¯‹à®¯à®¿à®²à¯',
                'à²­à²•à³à²¤à²¿', 'à²§à³à²¯à²¾à²¨', 'à²¸à²¾à²§à²¨à³†', 'à²¦à³ˆà²µà²¿à²•à²¤à³†', 'à²¦à³‡à²µà²°à³', 'à²ªà³‚à²œà³†', 'à²†à²°à²¾à²§à²¨à³†', 'à²¦à³‡à²µà²¾à²²à²¯',
                'à¤­à¤•à¥à¤¤à¥€', 'à¤ªà¥‚à¤œà¤¾', 'à¤¦à¥‡à¤µ', 'à¤§à¥à¤¯à¤¾à¤¨', 'à¤­à¤•à¥à¤¤', 'à¤¶à¥à¤°à¤¦à¥à¤§à¤¾',
                'à¦­à¦•à§à¦¤à¦¿', 'à¦†à¦¸à§à¦¥à¦¾', 'à¦†à¦§à§à¦¯à¦¾à¦¤à§à¦®à¦¿à¦•', 'à¦ªà§‚à¦œà¦¾', 'à¦ˆà¦¶à§à¦¬à¦°', 'à¦ªà§à¦°à¦¾à¦°à§à¦¥à¦¨à¦¾', 'à¦­à¦•à§à¦¤', 'à¦®à¦¨à§à¦¦à¦¿à¦°'
            ],
            'tragedy': [
                'à¤®à¥ƒà¤¤à¥à¤¯à¥', 'à¤µà¤¿à¤¨à¤¾à¤¶', 'à¤¦à¥à¤°à¥à¤­à¤¾à¤—à¥à¤¯', 'à¤•à¤·à¥à¤Ÿ', 'à¤†à¤ªà¤¤à¥à¤¤à¤¿', 'à¤¨à¤¾à¤¶', 'à¤µà¤¿à¤ªà¤¤à¥à¤¤à¤¿', 'à¤¹à¤¤à¥à¤¯à¤¾', 'à¤¯à¥à¤¦à¥à¤§',
                'à¤–à¥‚à¤¨', 'à¤¦à¥à¤°à¥à¤˜à¤Ÿà¤¨à¤¾', 'à¤¤à¥à¤°à¤¾à¤¸à¤¦à¥€', 'à¤®à¥Œà¤¤', 'à¤²à¤¾à¤¶', 'à¤¹à¤¿à¤‚à¤¸à¤¾', 'à¤¦à¤‚à¤—à¤¾', 'à¤ªà¥à¤°à¤²à¤¯', 'à¤¸à¤‚à¤¹à¤¾à¤°', 'à¤­à¤¯à¤¾à¤¨à¤•',
                'à¤®à¤°à¤£', 'à¤µà¤¿à¤ªà¤¦à¤¾', 'à¤†à¤ªà¤¦à¤¾', 'à¤­à¤¯à¤‚à¤•à¤°', 'à¤•à¥à¤°à¥‚à¤°', 'à¤•à¥à¤°à¥‚à¤°à¤¤à¤¾', 'à¤®à¤¾à¤°', 'à¤¤à¤¬à¤¾à¤¹à¥€',
                'à®‡à®±à®ªà¯à®ªà¯', 'à®¨à®¾à®šà®®à¯', 'à®µà®¿à®²à®•à¯à®•à®®à¯', 'à®šà¯‹à®•à®®à¯', 'à®•à¯Šà®²à¯ˆ', 'à®ªà¯‹à®°à¯', 'à®…à®´à®¿à®µà¯', 'à®ªà¯‡à®°à®´à®¿à®µà¯', 'à®®à®°à®£à®®à¯', 'à®ªà®Ÿà¯à®•à¯Šà®²à¯ˆ',
                'à²®à³ƒà²¤à³à²¯à³', 'à²¨à²¾à²¶', 'à²¦à³à²°à²‚à²¤', 'à²†à²ªà²¤à³à²¤à³', 'à²¯à³à²¦à³à²§', 'à²•à³Šà²²à³†', 'à²¨à²¾à²¶à²¨', 'à²¸à²¾à²µà³', 'à²¹à²¤à³à²¯à³†',
                'à¤®à¥ƒà¤¤à¥à¤¯à¥‚', 'à¤¨à¤¾à¤¶', 'à¤¦à¥à¤°à¥à¤˜à¤Ÿà¤¨à¤¾', 'à¤¯à¥à¤¦à¥à¤§', 'à¤¹à¤¤à¥à¤¯à¤¾', 'à¤®à¤°à¤£', 'à¤¸à¤‚à¤¹à¤¾à¤°',
                'à¦®à§ƒà¦¤à§à¦¯à§', 'à¦§à§à¦¬à¦‚à¦¸', 'à¦¦à§à¦°à§à¦­à¦¾à¦—à§à¦¯', 'à¦¯à§à¦¦à§à¦§', 'à¦¹à¦¤à§à¦¯à¦¾', 'à¦¬à¦¿à¦ªà¦°à§à¦¯à¦¯à¦¼', 'à¦°à¦•à§à¦¤', 'à¦®à§ƒà¦¤à¦¦à§‡à¦¹', 'à¦®à¦°à¦£', 'à¦¹à¦¤à§à¦¯à¦¾à¦•à¦¾à¦£à§à¦¡'
            ]
        }

    def _load_emotion_word_roots(self) -> Dict[str, List[str]]:
        return {
            'romance': ['à¤ªà¥à¤°à¥‡à¤®', 'à¤ªà¥à¤¯à¤¾à¤°', 'à®•à®¾à®¤à®²à¯', 'à²ªà³à²°à³‡à²®', 'à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸'],
            'melancholy': ['à¤¦à¥à¤–', 'à®¤à¯à®©à¯', 'à²¦à³à²ƒà²–', 'à¦¦à§à¦ƒà¦–'],
            'peace': ['à¤¶à¤¾à¤‚à¤¤', 'à®šà®¾à®¨à¯à®¤', 'à²¶à²¾à²‚à²¤', 'à¦¶à¦¾à¦¨à§à¦¤'],
            'joy': ['à¤–à¥à¤¶', 'à¤¹à¤°à¥à¤·', 'à®šà®¨à¯à®¤', 'à²¸à²‚à²¤à³‹à²·', 'à¦†à¦¨à¦¨à§à¦¦'],
            'inspiration': ['à¤¸à¤¾à¤¹à¤¸', 'à¤µà¥€à¤°', 'à®µà¯€à®°', 'à²§à³ˆà²°à³à²¯', 'à¦¸à¦¾à¦¹à¦¸'],
            'wisdom': ['à¤œà¥à¤žà¤¾à¤¨', 'à¤¬à¥à¤¦à¥à¤§à¤¿', 'à®žà®¾à®©', 'à²œà³à²žà²¾à²¨', 'à¦œà§à¦žà¦¾à¦¨'],
            'devotion': ['à¤­à¤•à¥à¤¤', 'à¤ªà¥‚à¤œà¤¾', 'à®ªà®•à¯à®¤', 'à²­à²•à³à²¤', 'à¦­à¦•à§à¦¤'],
            'tragedy': ['à¤®à¥ƒà¤¤à¥', 'à¤¨à¤¾à¤¶', 'à®®à®°à®£', 'à²®à³ƒà²¤à³à²¯à³', 'à¦®à§ƒà¦¤à§']
        }

    def _load_contextual_boosters(self) -> Dict[str, List[str]]:
        return {
            'wisdom': ['à¤®à¤¹à¤¾à¤•à¤¾à¤µà¥à¤¯', 'à¤¦à¤°à¥à¤¶à¤¨', 'à¤¤à¤¤à¥à¤¤à¥à¤µ', 'à¤µà¥‡à¤¦', 'à¤‰à¤ªà¤¨à¤¿à¤·à¤¦', 'à¤—à¥€à¤¤à¤¾', 'à®¤à®¤à¯à®¤à¯à®µà®®', 'à²¤à²¤à³à²µ'],
            'devotion': ['à¤­à¤—à¤µà¤¾à¤¨', 'à¤ˆà¤¶à¥à¤µà¤°', 'à¤•à¥ƒà¤·à¥à¤£', 'à¤°à¤¾à¤®', 'à®¤à¯†à®¯à¯à®µà®®', 'à²¦à³‡à²µ'],
            'tragedy': ['à¤¯à¥à¤¦à¥à¤§', 'à¤ªà¥à¤°à¤²à¤¯', 'à¤µà¤¿à¤¨à¤¾à¤¶', 'à®ªà¯‹à®°à¯', 'à²¯à³à²¦à³à²§'],
            'inspiration': ['à¤µà¥€à¤°', 'à¤®à¤¹à¤¾à¤¨', 'à¦¬à§€à¦°', 'à®µà¯€à®°', 'à²µà³€à²°']
        }

    def _load_language_indicators(self) -> Dict[str, List[str]]:
        return {
            'Hindi':   ['à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥ƒ', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥Œ', 'à¤‚', 'à¤ƒ'],
            'Marathi': ['à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥ƒ', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥Œ', 'à¤‚', 'à¤ƒ'],
            'Tamil':   ['à¯', 'à®¾', 'à®¿', 'à¯€', 'à¯', 'à¯‚', 'à¯†', 'à¯‡', 'à¯ˆ', 'à¯‹', 'à¯Œ', 'à®‚'],
            'Kannada': ['à³', 'à²¾', 'à²¿', 'à³€', 'à³', 'à³‚', 'à³ƒ', 'à³†', 'à³‡', 'à³ˆ', 'à³Š', 'à³‹', 'à³Œ', 'à²‚'],
            'Bengali': ['à¦¾', 'à¦¿', 'à§€', 'à§', 'à§‚', 'à§ƒ', 'à§‡', 'à§ˆ', 'à§‹', 'à§Œ', 'à¦‚', 'à¦ƒ']
        }

    def preprocess_text(self, text: str) -> str:
        text = re.sub(r'[à¥¤à¥¥,.\-!?;:\'\"()\[\]{}]', ' ', text)
        words = [w for w in text.split() if w not in self.stop_words and len(w) > 1]
        return ' '.join(words)

    def get_text_embedding(self, text: str):
        """âœ… PRE-TRAINED MODEL: Get contextualized embedding using IndicBERT"""
        if self.tokenizer is None or self.model is None:
            return None
        
        try:
            inputs = self.tokenizer(text, return_tensors="pt", 
                                   truncation=True, max_length=512, padding=True)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embedding = outputs.last_hidden_state[:, 0, :].squeeze()
            
            return embedding.numpy()
        except Exception as e:
            return None

    def extract_emotion_scores(self, text: str) -> Dict[str, float]:
        """
        âœ… MODEL-FIRST EMOTION DETECTION
        
        Priority:
        1. IndicBERT embeddings (70% weight) - PRIMARY
        2. Keyword matching (20% weight) - SECONDARY
        3. Context boosters (10% weight) - TERTIARY
        """
        
        keyword_scores = {e: 0.0 for e in self.emotion_keywords.keys()}
        model_scores = {e: 0.0 for e in self.emotion_keywords.keys()}
        
        # ================== STRATEGY 1: IndicBERT (70%) ==================
        if self.model is not None and self.tokenizer is not None:
            embedding = self.get_text_embedding(text[:512])
            
            if embedding is not None:
                emotion_embeddings = {}
                
                for emotion, keywords in self.emotion_keywords.items():
                    sample_keywords = ' '.join(keywords[:10])
                    emotion_emb = self.get_text_embedding(sample_keywords)
                    
                    if emotion_emb is not None:
                        emotion_embeddings[emotion] = emotion_emb
                
                for emotion, emotion_emb in emotion_embeddings.items():
                    try:
                        similarity = np.dot(embedding, emotion_emb) / (
                            np.linalg.norm(embedding) * np.linalg.norm(emotion_emb)
                        )
                        model_scores[emotion] = max(0, (similarity + 1) / 2) * 10.0
                    except:
                        model_scores[emotion] = 0.5
        
        # ================== STRATEGY 2: Keywords (20%) ==================
        for emotion, keywords in self.emotion_keywords.items():
            for keyword in keywords:
                count = text.count(keyword)
                if count > 0:
                    keyword_scores[emotion] += count * 0.5
        
        for emotion, roots in self.emotion_word_roots.items():
            for root in roots:
                for word in text.split():
                    if root in word and len(root) >= 3:
                        keyword_scores[emotion] += 0.3
        
        # ================== STRATEGY 3: Boosters (10%) ==================
        booster_scores = {e: 0.0 for e in self.emotion_keywords.keys()}
        for emotion, boosters in self.contextual_boosters.items():
            for booster in boosters:
                if booster in text:
                    booster_scores[emotion] += 0.5
        
        # ================== WEIGHTED COMBINATION ==================
        if self.model is not None and sum(model_scores.values()) > 0:
            final_scores = {}
            for emotion in self.emotion_keywords.keys():
                final_scores[emotion] = (
                    model_scores[emotion] * 0.70 +
                    keyword_scores[emotion] * 0.20 +
                    booster_scores[emotion] * 0.10
                )
        else:
            final_scores = {}
            for emotion in self.emotion_keywords.keys():
                final_scores[emotion] = (
                    keyword_scores[emotion] * 0.70 +
                    booster_scores[emotion] * 0.30 +
                    0.5
                )
        
        # Normalize
        total = sum(final_scores.values())
        if total == 0:
            return {e: 1/len(final_scores) for e in final_scores.keys()}
        
        return {e: s / total for e, s in final_scores.items()}

    def detect_language(self, text: str) -> str:
        text_sample = text[:200]
        lang_scores = {}
        
        for language, indicators in self.language_indicators.items():
            count = sum(text_sample.count(ind) for ind in indicators)
            lang_scores[language] = count
        
        detected = max(lang_scores, key=lang_scores.get)
        return detected if lang_scores[detected] > 0 else 'Hindi'

    def semantic_search(self, query: str, texts: List[str], top_k: int = 5) -> List[Tuple[int, float]]:
        try:
            all_texts = [query] + texts
            tfidf_matrix = self.vectorizer.fit_transform(all_texts)
            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]
            
            top_indices = np.argsort(similarities)[::-1][:top_k]
            return [(int(idx), float(similarities[idx])) for idx in top_indices]
        except Exception as e:
            return []

    def generate_suggestions(self, query: str, book_titles: List[str]) -> List[str]:
        matches = [t for t in book_titles if query.lower() in t.lower()]
        return matches[:5]

    def extract_phrases(self, text: str) -> List[str]:
        words = text.split()
        phrases = []
        
        for i in range(len(words) - 1):
            phrases.append(' '.join(words[i:i+2]))
            if i < len(words) - 2:
                phrases.append(' '.join(words[i:i+3]))
        
        phrase_counts = Counter(phrases)
        return [p for p, _ in phrase_counts.most_common(10)]
